{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.lang.en import English\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def analyze_text(text):\n",
    "\treturn nlp(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  [0, 1, 2, 3, 4, 5]\n",
      "Orth: [7859011591137717335, 2772908615185555060, 11283501755624150392, 5533571732986600803, 17365054503653917826, 12646065887601541794]\n",
      "Text:  ['It', 'costs', '$', '1', 'million', '.']\n",
      "is_alpha:  [True, True, False, False, True, False]\n",
      "is_punct:  [False, False, False, False, False, True]\n",
      "like_num:  [False, False, False, True, True, False]\n"
     ]
    }
   ],
   "source": [
    "# # # Tokenization Explained # # #\n",
    "doc = analyze_text('It costs $1 million.')\n",
    "print(\"Index: \", [token.i for token in doc])\n",
    "print(\"Orth:\", [token.orth for token in doc]) \n",
    "print(\"Text: \", [token.text for token in doc]) \n",
    "print(\"is_alpha: \", [token.is_alpha for token in doc])\n",
    "print(\"is_punct: \", [token.is_punct for token in doc])\n",
    "print(\"like_num: \", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # spaCy Labels Explained # # #\n",
    "def spacy_explain(label):\n",
    "\tprint(spacy.explain(label))\n",
    "\n",
    "\n",
    "spacy_explain('DET')\n",
    "spacy_explain('nsubj')\n",
    "spacy_explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Linguistic Features # # #\n",
    "doc = analyze_text('John Smith ate a pizza yesterday!')\n",
    "\n",
    "for token in doc:\n",
    "    # The part-of-speech tag of the token head.\n",
    "    print('Part of speech', token.text, '-->', token.pos_)\n",
    "    # The syntactic relation connecting child to head.\n",
    "    print('Dependency parser', token.text, '-->', token.dep_)\n",
    "    # The original text of the token head.\n",
    "    print('Head Text', token.head.text, '-->', token.text)\n",
    "\n",
    "\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Predicting Named Entities # # #\n",
    "\n",
    "doc = analyze_text('Apple is looking at buying U.K. startup for $1 Billion')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, '-->', ent.label_)\n",
    "\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # LEMMA attribute # # #\n",
    "# Lemma (Finds the root word \"win\" -> [\"won\", \"winning\", \"win\"]), \n",
    "# (Stemming “winning” -> “winn” and that’s not an english word!)\n",
    "\n",
    "def lemmatisation():\n",
    "    doc = analyze_text('winning won wins has')\n",
    "    print(\"Text -->\", [token.text for token in doc])\n",
    "    print(\"Lemma -->\", [token.lemma_ for token in doc])\n",
    "\n",
    "lemmatisation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Efficient Phrase Matcher # # #\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sportsPatterns(sport):\n",
    "\treturn [\n",
    "\t\t{'IS_DIGIT': True},\n",
    "\t\t{'LOWER': f'{sport}', 'OP': '?'},\n",
    "\t\t{'LOWER': 'world'},\n",
    "\t\t{'LOWER': 'cup'},\n",
    "\t\t{'IS_PUNCT': True},\n",
    "\t]\n",
    "\n",
    "\n",
    "def emotionPatterns(emotion):\n",
    "\treturn [\n",
    "\t\t{'LEMMA': f'{emotion}', 'POS': 'VERB'},\n",
    "\t]\n",
    "\n",
    "\n",
    "def gadgetPatterns(gadget, extensionName):\n",
    "\treturn [\n",
    "\t\t{'LOWER': f'{gadget}'}, \n",
    "\t\t{'LOWER': f'{extensionName}', 'OP': '?'}\n",
    "\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa = sportsPatterns(sport='fifa')\n",
    "rugby = sportsPatterns(sport='rugby')\n",
    "\n",
    "love = emotionPatterns(emotion='love')\n",
    "hate = emotionPatterns(emotion='hate')\n",
    "\n",
    "phone = gadgetPatterns(gadget='iphone', extensionName='x')\n",
    "computer = gadgetPatterns(gadget='mac', extensionName=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_matchers(matcher):\n",
    "\tmatcher.add(\"World_Cups\", [fifa, rugby]);\n",
    "\tmatcher.add(\"Emotion\", [love, hate]);\n",
    "\tmatcher.add(\"Gadgets\", [phone, computer]);\n",
    "\n",
    "\n",
    "add_matchers(matcher)\n",
    "\n",
    "def showMatcher(doc):\n",
    "\tmatches = matcher(doc)\n",
    "\tfor match_id, start, end in matches:\n",
    "\t\tstring_id = nlp.vocab.strings[match_id]  # Get string representation of matcher\n",
    "\t\tspan = doc[start:end]  # The matched span\n",
    "\t\tprint(\n",
    "\t\t\tf\"\"\"match_id: {match_id},\n",
    "string_id: {string_id},\n",
    "start: {start},\n",
    "end: {end},\n",
    "TEXT: {span.text}\n",
    "\t\t\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showRepresentationOfMatchers():\n",
    "\tdoc = analyze_text('Upcoming Mac Pro, has leaked the release date')\n",
    "\tdoc2 = analyze_text('2018 FIFA world cup: France won!')\n",
    "\tdoc3 = analyze_text('I loved dogs now I love cats more')\n",
    "\tdoc4 = analyze_text('I hate tomatoes')\n",
    "\tshowMatcher(doc)\n",
    "\tshowMatcher(doc2)\n",
    "\tshowMatcher(doc3)\n",
    "\tshowMatcher(doc4)\n",
    "\n",
    "\n",
    "showRepresentationOfMatchers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Efficient Phrase Matcher # # #\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "pattern = analyze_text('Golden Retriever')\n",
    "pattern2 = analyze_text('Golden Retriever')\n",
    "matcher.add('DOG', [pattern, pattern2])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched phrase: ', span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Similarity # # #\n",
    "\n",
    "# # 2 documents\n",
    "doc1 = analyze_text('I like fast food')\n",
    "doc2 = analyze_text('I love pizza')\n",
    "print(f\"{round(doc1.similarity(doc2) * 100, 2)}%\")\n",
    "\n",
    "# # 2 tokens\n",
    "doc = analyze_text('I like pizza and pasta')\n",
    "print(f\"{round(doc[2].similarity(doc[4]) * 100, 2)}%\")\n",
    "\n",
    "# # document and token\n",
    "doc3 = analyze_text('I love pizza')\n",
    "token = analyze_text('soap')[0]\n",
    "print(f\"{round(doc3.similarity(token) * 100, 2)}%\")\n",
    "\n",
    "# span and document\n",
    "span = analyze_text('I like pizza and pasta')[2: 5]\n",
    "document = analyze_text('MacDonald\\'s sells burgers')\n",
    "\n",
    "print(f\"{round(span.similarity(document) * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python385jvsc74a57bd0fdb6e4e95dceca98dc5bdeb0584ed74dea34b8c166b4139088771187aee49efb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}