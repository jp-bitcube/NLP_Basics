{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Import Spacy</h1>\n",
    "<H4>N.L.P in spaCy is the Natural Language Process</h4>\n",
    "<H4>Ultimately Human language computerized into to machines...</h4>\n",
    "<H4>Linguistics is the Science of Language</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Loading a spaCy Model ( Language ) i.e. 'en_core_web_lg' </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Tokenization<h2>& derived properties of words.</h2></H1> \n",
    "<h2>The computer understanding the word as we would</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenization(text):\n",
    "\treturn nlp(text)\n",
    "\n",
    "doc = word_tokenization('It costs £1 million pounds.')\n",
    "print(\"Index: \", [token.i for token in doc])\n",
    "print(\"Orth: \", [token.orth for token in doc]) \n",
    "print(\"Text: \", [token.text for token in doc]) \n",
    "print(\"is_alpha: \", [token.is_alpha for token in doc])\n",
    "print(\"is_punct: \", [token.is_punct for token in doc])\n",
    "print(\"like_num: \", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>spaCy Labels Explained</H1>\n",
    "\n",
    "\"Charlie\" said to me, 'Wow!, Really spaCy can explain its labels but How?'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_explain(label):\n",
    "\tprint(spacy.explain(label))\n",
    "\n",
    "\n",
    "spacy_explain('ADJ')\n",
    "spacy_explain('nsubj')\n",
    "spacy_explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Basic Linguistic Features</H1>\n",
    "\n",
    "<h3>P.O.S --> Parts of speech</h3>\n",
    "<h3>Dependency Parser --> The syntactic relation connecting child to head</h3>\n",
    "<h3>Head of Text --> The original text of the token head.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = word_tokenization('I had a great time at the beach.') # 'At the beach, I had a great time', 'I had, at the beach, a great time'\n",
    "\n",
    "for token in doc:\n",
    "    print('P.O.S', token.text, '-->', token.pos_)\n",
    "    print('Dependency', token.text, '-->', token.dep_)\n",
    "    print('Head of Text', token.head.text, '-->', token.text, ',')\n",
    "\n",
    "\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_explain('prep')\n",
    "spacy_explain('dobj')\n",
    "spacy_explain('pobj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Predicting Named Entities</H1>\n",
    "<H3>spaCy's pre-trained entities</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = word_tokenization('\"Apple is looking at buying U.K. startup for £1 billion.\" Charlie said the yesterday')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, '-->', ent.label_)\n",
    "\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_explain('ORG')\n",
    "spacy_explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>LEMMA attribute</H1>\n",
    "<H2>The goal of both stemming and lemmatization is to reduce inflectional forms<H2>\n",
    "<H2>Lemmatization --> Finds the root word [ am, are, is ] --> be</H2>\n",
    "<H2>Stemming --> Removes inflectional form of words in this case the prefix of --> [ 'winning' ] --> [ 'winn' ] (Not a real word)</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization():\n",
    "    doc = word_tokenization('am are is')\n",
    "    print(\"Text -->\", [token.text for token in doc])\n",
    "    print(\"Lemma -->\", [token.lemma_ for token in doc])\n",
    "\n",
    "lemmatization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>spaCy Matcher Class</H1>\n",
    "<H2>Matcher --> Uses spaCy's Linguistic features and Lexical properties to look for phrases, words etc</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Creating Patterns for the Matcher<H1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sportsPatterns(sport):\n",
    "\treturn [\n",
    "\t\t{'IS_DIGIT': True},\n",
    "\t\t{'LOWER': f'{sport}', 'OP': '?'},\n",
    "\t\t{'LOWER': 'world'},\n",
    "\t\t{'LOWER': 'cup'},\n",
    "\t\t{'IS_PUNCT': True},\n",
    "\t]\n",
    "\n",
    "\n",
    "def emotionPatterns(emotion):\n",
    "\treturn [\n",
    "\t\t{'LEMMA': f'{emotion}', 'POS': 'VERB'},\n",
    "\t]\n",
    "\n",
    "\n",
    "def gadgetPatterns(gadget, extensionName):\n",
    "\treturn [\n",
    "\t\t{'LOWER': f'{gadget}'}, \n",
    "\t\t{'LOWER': f'{extensionName}', 'OP': '?'}\n",
    "\t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Setting a few variable patterns based on a category</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa = sportsPatterns(sport='fifa')\n",
    "rugby = sportsPatterns(sport='rugby')\n",
    "\n",
    "love = emotionPatterns(emotion='love')\n",
    "hate = emotionPatterns(emotion='hate')\n",
    "\n",
    "phone = gadgetPatterns(gadget='iphone', extensionName='x')\n",
    "computer = gadgetPatterns(gadget='mac', extensionName=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Adding Matchers and Set up away to show the matcher</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_matchers(matcher):\n",
    "\tmatcher.add(\"World_Cups\", [fifa, rugby]);\n",
    "\tmatcher.add(\"Emotion\", [love, hate]);\n",
    "\tmatcher.add(\"Gadgets\", [phone, computer]);\n",
    "\n",
    "\n",
    "add_matchers(matcher)\n",
    "\n",
    "def showMatcher(doc):\n",
    "\tmatches = matcher(doc)\n",
    "\tfor match_id, start, end in matches:\n",
    "\t\tstring_id = nlp.vocab.strings[match_id]  # Get string representation of matcher\n",
    "\t\tspan = doc[start:end]  # The matched span\n",
    "\t\tprint(\n",
    "\t\t\tf\"\"\"\n",
    "match_id: {match_id},\n",
    "string_id: {string_id},\n",
    "start: {start},\n",
    "end: {end},\n",
    "TEXT: {span.text}\n",
    "\t\t\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Showing Matchers In Action</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showRepresentationOfMatchers():\n",
    "\tdoc = word_tokenization('Upcoming Mac Pro, has leaked the release date')\n",
    "\tdoc2 = word_tokenization('2018 FIFA world cup: France won!')\n",
    "\tdoc3 = word_tokenization('I loved dogs now I love cats more')\n",
    "\tdoc4 = word_tokenization('I hate tomatoes')\n",
    "\tshowMatcher(doc)\n",
    "\tshowMatcher(doc2)\n",
    "\tshowMatcher(doc3)\n",
    "\tshowMatcher(doc4)\n",
    "\n",
    "\n",
    "showRepresentationOfMatchers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Efficient Phrase Matcher</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "pattern = word_tokenization('Golden Retriever')\n",
    "pattern2 = word_tokenization('golden retriever')\n",
    "pattern3 = word_tokenization('lion')\n",
    "pattern4 = word_tokenization('Tiger')\n",
    "matcher.add('DOG', [pattern, pattern2])\n",
    "matcher.add('CAT', [pattern3, pattern4])\n",
    "\n",
    "doc = nlp(\"Me and my Golden Retriever, saw a lion at the Zoo\")\n",
    "\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print('Matched phrase: ', span.text)\n",
    "    print('Matcher: ', string_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Basics Similarity && Vectors</H1>\n",
    "<H2>Similarity ---> Default calculates cosine similarity<H2>\n",
    "<H2>Vectors/Word embeddings ---> Comparison technique used by spaCy to give a multi-dimensional representation of the word<H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2 documents\n",
    "# doc1 = word_tokenization('I like fast food')\n",
    "# doc2 = word_tokenization('I love pizza')\n",
    "# print(f\"{round(doc1.similarity(doc2) * 100, 2)}%\")\n",
    "\n",
    "# # 2 tokens\n",
    "# doc = word_tokenization('I like pizza and pasta')\n",
    "# print(f\"{round(doc[2].similarity(doc[4]) * 100, 2)}%\")\n",
    "\n",
    "# # document and token\n",
    "# doc3 = word_tokenization('I love pizza')\n",
    "# token = word_tokenization('soap')[0]\n",
    "# print(f\"{round(doc3.similarity(token) * 100, 2)}%\")\n",
    "\n",
    "# span and document\n",
    "# span = word_tokenization('I like pizza and pasta')[2: 5]\n",
    "# document = word_tokenization('MacDonald\\'s sells burgers')\n",
    "\n",
    "# print(f\"{round(span.similarity(document) * 100, 2)}%\")\n",
    "\n",
    "# print(f\"Length for doc1 vectors is {len(doc1.vector)}\")\n",
    "# print(f\"Length for doc2 vectors is {len(doc2.vector)}\")\n",
    "# print(f\"Vectors for doc1 {doc1.vector[0: 20]}\")\n",
    "# print(f\"Vectors for doc2 {doc2.vector[0: 20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "596da59c3b517429896be298a7a92193c91659930c63f247802a5246489e9278"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5  ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}